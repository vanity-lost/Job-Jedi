{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, request\n",
    "import openai\n",
    "import os\n",
    "import langchain\n",
    "import os\n",
    "from langchain.llms import OpenAI\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.chains import LLMChain, APIChain, ConversationChain\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.text_splitter import TextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain.text_splitter import CharacterTextSplitter, TextSplitter\n",
    "from langchain.vectorstores import ElasticVectorSearch, Pinecone, Weaviate, FAISS\n",
    "from langchain.memory import ConversationSummaryMemory\n",
    "from langchain.chains.summarize import load_summarize_chain\n",
    "from langchain.docstore.document import Document\n",
    "from langchain.chains.mapreduce import MapReduceChain\n",
    "from langchain.document_loaders import Docx2txtLoader\n",
    "from langchain.callbacks import get_openai_callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "REPHRASE_PROMPT_TEMPLATE = \"\"\"With the background information \\n {background} \\n, now you are given the detailed job description \\n {job} \\n.\n",
    "    Based on the background information and job description, generate a strong resume.\n",
    "    Using the job description, select ONLY the top three related strong experience/projects as the background information, rephrase/summarize background to be much stronger and more match to the job. Keep at least three expriences and also project remained.\n",
    "    DO NOT introduce non-existing information. Pay attention to deliberately exclude any unnecessary pieces of information in the generated resume. The generated resume should have length around 500 tokens, i.e. keep the resume concise on language and detailed in experience.\n",
    "    \n",
    "    generated resume:\"\"\"\n",
    "\n",
    "LATEX_PROMPT_TEMPLAT_1 = \"\"\"With the background information {background}, now you are given the latex template {template}.\n",
    "    Based on the background information, fill corresonding information into the heading, education, and experience of the latex template.\n",
    "    Using the background information, modifying as little as possible, keep the latex structure as unchanged, only modifying the text part, and generate a one-page latex pdf. \n",
    "    The experience section of the latex template refers to the working experience. The generated resume should ONLY include the heading, education, and the experience. Pay attention to deliberately exclude any unnecessary pieces of information in the generated resume: DO NOT include any projects and/or skills.\n",
    "    Remember that in latex, we put the backslash '\\' before the special characters, such as '#'. Use only utf-8 characters.\n",
    "    \n",
    "    generated resume:\"\"\"\n",
    "\n",
    "LATEX_PROMPT_TEMPLAT_2 = \"\"\"With the background information {background}, now you are given the latex template {template}.\n",
    "    Based on the background information, fill corresonding information into the projects, programming skills, and ending of the latex template.\n",
    "    Using the background information, modifying as little as possible, keep the latex structure as unchanged, only modifying the text part, and generate a one-page latex pdf. \n",
    "    The projects section of the latex template refers to the projects experience. The generated resume should ONLY include the projects, programming skills, and ending. Pay attention to deliberately exclude any unnecessary pieces of information in the generated resume: DO NOT include any heading, education and/or working experience.\n",
    "    Remember that in latex, we put the backslash '\\' before the special characters, such as '#' (e.g. C# should write as C\\#).\n",
    "    Use only utf-8 characters.\n",
    "    Remeber that you are writing the second half part of the resume, so:\n",
    "    You must include the \\end document code to make sure the latex code is end.\n",
    "    You must not include the \\documentclass and \\begin docment in the result. \n",
    "\n",
    "    generated resume:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./resources/header.tex') as f:\n",
    "    latex_header = f.read()\n",
    "\n",
    "with open('./resume/part1.tex') as f:\n",
    "    part1 = f.read()\n",
    "\n",
    "with open('./resume/part2.tex') as f:\n",
    "    part2 = f.read()\n",
    "\n",
    "llm = OpenAI(model_name='text-davinci-003', max_tokens=1200)\n",
    "\n",
    "background_path = './resources/Resume Materials.docx'\n",
    "\n",
    "loader = Docx2txtLoader(background_path)\n",
    "background_info = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_splitter = CharacterTextSplitter(chunk_size=4000, chunk_overlap=0)\n",
    "\n",
    "background_info_doc = text_splitter.split_documents(background_info)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 2953\n",
      "\tPrompt Tokens: 2058\n",
      "\tCompletion Tokens: 895\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.059059999999999994\n",
      "Tokens Used: 2488\n",
      "\tPrompt Tokens: 1749\n",
      "\tCompletion Tokens: 739\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.049760000000000006\n"
     ]
    }
   ],
   "source": [
    "rephrase_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"background\",\n",
    "        \"job\",\n",
    "    ],\n",
    "    template=REPHRASE_PROMPT_TEMPLATE,\n",
    ")\n",
    "\n",
    "rephrase_chain = LLMChain(llm=llm,\n",
    "                          prompt=rephrase_template,\n",
    "                          verbose=False)\n",
    "\n",
    "results = []\n",
    "for part in background_info_doc:\n",
    "    with get_openai_callback() as cb:\n",
    "        res = rephrase_chain.run(background=part, job=job_description)\n",
    "        results.append(res)\n",
    "        print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "background_info_generated = '\\n\\n'.join(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tokens Used: 3530\n",
      "\tPrompt Tokens: 2746\n",
      "\tCompletion Tokens: 784\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.0706\n",
      "Tokens Used: 3042\n",
      "\tPrompt Tokens: 2246\n",
      "\tCompletion Tokens: 796\n",
      "Successful Requests: 1\n",
      "Total Cost (USD): $0.060840000000000005\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "reformat_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"template\",\n",
    "        \"background\",\n",
    "    ],\n",
    "    template=LATEX_PROMPT_TEMPLAT_1,\n",
    ")\n",
    "\n",
    "reformat_chain = LLMChain(llm=llm, \n",
    "                          prompt=reformat_template,\n",
    "                          verbose=False)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    res = reformat_chain.run(background=background_info_generated, template=part1)\n",
    "    results.append(bytes(res, 'utf-8').decode('utf-8', 'ignore').strip())\n",
    "    print(cb)\n",
    "\n",
    "reformat_template = PromptTemplate(\n",
    "    input_variables=[\n",
    "        \"template\",\n",
    "        \"background\",\n",
    "    ],\n",
    "    template=LATEX_PROMPT_TEMPLAT_2,\n",
    ")\n",
    "\n",
    "reformat_chain = LLMChain(llm=llm, \n",
    "                          prompt=reformat_template,\n",
    "                          verbose=False)\n",
    "\n",
    "with get_openai_callback() as cb:\n",
    "    res = reformat_chain.run(background=background_info_generated, template=part2)\n",
    "    results.append(bytes(res, 'utf-8').decode('utf-8', 'ignore').strip())\n",
    "    print(cb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_resume = latex_header + '\\n' + results[0] + '\\n' + results[1]\n",
    "\n",
    "with open('./resume/resume.tex', 'w', encoding='utf-8') as f:\n",
    "    f.write(generated_resume)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.system(\"pdflatex ./resume/resume.tex\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHAT_PROMPT_TEMPLAT = \"\"\"Based on the user request, identify whether the user is requesting to modify the resume.\n",
    "    You are only allowed to answer in 'YES' or 'NO'. Do not include any other information or responses.\n",
    "\n",
    "    User Request:{query}\n",
    "    Response:\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'YES'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_query = 'regenerate'\n",
    "\n",
    "with open('./prompts/chat_template.txt') as f:\n",
    "    CHAT_PROMPT_TEMPLAT = f.read()\n",
    "\n",
    "chat_template = PromptTemplate(\n",
    "    input_variables=[\"query\"],\n",
    "    template=CHAT_PROMPT_TEMPLAT,\n",
    ")\n",
    "\n",
    "chat_memory = ConversationBufferMemory(input_key='query', memory_key='query_history')\n",
    "\n",
    "conversation = LLMChain(llm=OpenAI(temperature=0), memory=chat_memory, prompt=chat_template)\n",
    "\n",
    "res = conversation.run(query=user_query).strip()\n",
    "\n",
    "res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./prompts/revision_template.txt') as f:\n",
    "    REVISE_PROMPT_TEMPLAT = f.read()\n",
    "\n",
    "revise_template = PromptTemplate(\n",
    "    input_variables=[\"old_template\", \"user_request\"],\n",
    "    template=REVISE_PROMPT_TEMPLAT,\n",
    ")\n",
    "\n",
    "with open('./prompts/rephrase_template.txt') as f:\n",
    "    REPHRASE_PROMPT_TEMPLATE = f.read().replace('{', '#(#').replace('}', '#)#')\n",
    "with open('./prompts/latex_template_1.txt') as f:\n",
    "    LATEX_PROMPT_TEMPLAT_1 = f.read().replace('{', '#(#').replace('}', '#)#')\n",
    "with open('./prompts/latex_template_2.txt') as f:\n",
    "    LATEX_PROMPT_TEMPLAT_2 = f.read().replace('{', '#(#').replace('}', '#)#')\n",
    "\n",
    "revision_chain = LLMChain(llm=OpenAI(temperature=0.7), \n",
    "                          prompt=revise_template,\n",
    "                          verbose=False)\n",
    "\n",
    "REPHRASE_PROMPT_TEMPLATE = revision_chain.run(old_template=REPHRASE_PROMPT_TEMPLATE, user_request=user_query)\n",
    "LATEX_PROMPT_TEMPLAT_1 = revision_chain.run(old_template=LATEX_PROMPT_TEMPLAT_1, user_request=user_query)\n",
    "LATEX_PROMPT_TEMPLAT_2 = revision_chain.run(old_template=LATEX_PROMPT_TEMPLAT_2, user_request=user_query)\n",
    "\n",
    "with open('./prompts/rephrase_template.txt', 'w') as f:\n",
    "    f.write(REPHRASE_PROMPT_TEMPLATE.replace('#(#', '{').replace('#)#', '}'))\n",
    "with open('./prompts/latex_template_1.txt', 'w') as f:\n",
    "    f.write(LATEX_PROMPT_TEMPLAT_1.replace('#(#', '{').replace('#)#', '}'))\n",
    "with open('./prompts/latex_template_2.txt', 'w') as f:\n",
    "    f.write(LATEX_PROMPT_TEMPLAT_2.replace('#(#', '{').replace('#)#', '}'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "570feb405e2e27c949193ac68f46852414290d515b0ba6e5d90d076ed2284471"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
